---
title: "Activity Prediction Using Groupware Data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

One of the interestng things is to gather data about individuals' excercise and analyze the manner in which they did the exercise. Using modern gadgets such as Jawbone Up, Nike FuelBand, and Fitbit facilitates collecting large amounts of data about personal activity. In this report we will data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The goal to predict the manner in which they did the exercise. 

# Dataset

The data for this project is provided by [Groupware]( http://groupware.les.inf.puc-rio.br/har). Training set is available [Here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and Test set is available [Here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).
The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways, while placing accelerometers on the belt, forearm, arm, and dumbell

# Data Exploration And Cleaning

## Loading Libraries

```{r warning=FALSE,message=FALSE}
#load libraries
library(caret)
library(randomForest)
library(rpart)
library(dplyr)
library(rattle)
```
## Exploring Data

```{r}
traind<-read.csv("./data/pml-training.csv",
              stringsAsFactors = FALSE,
              header = TRUE,
              na.strings = c("","NA"))

testd<-read.csv("./data/pml-testing.csv",
                stringsAsFactors = FALSE,
                header = TRUE,
                na.strings = c("","NA"))

dim(traind)
```
We can see that we have 160 column, including the **classe** variable which we want to predict in the test set.

## Cleaning Data

Here we will:
- remove any columns that include more than 80% Nas.

```{r}
#check percentage of NAs
check_na<-sapply(1:dim(traind)[2], function(x) mean(is.na(traind[,x])))

#get columns with less than 20% Na
traind<- traind[,which(check_na<0.2)]
testd<- testd[,which(check_na<0.2)]

```

- remove columns that include ids and data that do not come from measurements, and won't help in our predictions.
```{r}
#remove the first 7 columns as they seem to have no effect (id, name..etc.)
traind<-traind[,8:dim(traind)[2]]
testd<-testd[,8:dim(testd)[2]]
```

- convert the **classe** variable into factor as it is our output variable.
```{r}
traind$classe<-as.factor(traind$classe)
```

# Prediction

## Data Partitioning 
 
Here we'll devide the training set into 2 parts 60%, 40%

```{r}
##divide training set into 2 parts 60%, 40%
inTrain<-createDataPartition(y=traind$classe,p=0.6,list = F)

training <- traind[inTrain,]
testing <- traind[-inTrain,]

```

## Decision Tree
Here we will start with a basic classification tree

```{r}
model<-train(classe ~ .,data = training, method = "rpart")

fancyRpartPlot(model$finalModel, sub="")
```

If we check confusion matrix, we can see that the result is unsatisfactory, the accuracy is low. So we'll try a random forest.
```{r}
confusionMatrix(testing$classe, predict(model, testing))
```

## Random Forest
Here we will fit a random forest and check the accuracy.

```{r}
model2<- randomForest(classe ~ ., data = training)

predict_class<-predict(model2, testing, type = "class")
```

Let's look at the confusion matrix

```{r}
confusionMatrix(testing$classe,predict_class)
```

It seems that the random forest gave a good level of accuracy that we can rely on. So we'll use our test set to predict **classe**.

```{r}
p<-predict(model2, testd, type = "class")

```

And here is the predicition for the  20 test cases:
```{r, echo=FALSE}
data.frame(p)
```